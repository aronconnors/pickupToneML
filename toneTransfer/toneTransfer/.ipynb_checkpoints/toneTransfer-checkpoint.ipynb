{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cae3bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#do ya imports and shiii\n",
    "\n",
    "#!pip install ddsp[training]\n",
    "import ddsp\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4ef79",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from ddsp.training.data_preparation import prepare_tfrecord_lib\n",
    "\n",
    "input_files = [\n",
    "    \"Sounds/singlecoil/*.wav\",\n",
    "    \"Sounds/humbucker/*.wav\"\n",
    "]\n",
    "\n",
    "prepare_tfrecord_lib.prepare_tfrecord(\n",
    "    input_audio_filepatterns=input_files,\n",
    "    output_tfrecord_path=\"guitar_dataset.tfrecord\",\n",
    "    num_shards=1,\n",
    "    sample_rate=16000, #SAMPLE_RATE PERAMETER?\n",
    "    frame_rate=250,\n",
    "    example_secs=4.0, #EXAMPLE_SECS PERAMETER?\n",
    "    hop_secs=1.0,\n",
    "    eval_split_fraction=0.0,\n",
    "    chunk_secs=20.0\n",
    ")\n",
    "\n",
    "\n",
    "# this is how you preprocess the data, lmao, its the actual code from the gitRepo\n",
    "#heres that: https://github.com/magenta/ddsp/blob/main/ddsp/training/data_preparation/ddsp_prepare_tfrecord.py\n",
    "\n",
    "#this is the same as using google's library like this in the command line\n",
    "\n",
    "'''ddsp_prepare_tfrecord \\\n",
    "  --input_audio_filepatterns=\"/full/path/to/guitar_data/*/*.wav\" \\\n",
    "  --output_tfrecord_path=\"/full/path/to/output/guitar_dataset.tfrecord\" \\\n",
    "  --num_shards=1\n",
    "'''\n",
    "#idrk what a shard is\n",
    "\n",
    "'''This contains:\n",
    "\n",
    "    The waveform\n",
    "\n",
    "    The extracted pitch\n",
    "\n",
    "    Loudness\n",
    "\n",
    "    Other metadata\n",
    "\n",
    "This file will be your training data.'''\n",
    "\n",
    "#this is kinda like a train-test-split, but a step before that acually happens\n",
    "# its a \"preprocess data\" function. we love google\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c8b4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#convert the MOTHAFUKIN data from a preprocessed .tfrecord file to a tensorflow guitar_dataset\n",
    "\n",
    "from ddsp.training.data import TFRecordProvider\n",
    "\n",
    "provider = TFRecordProvider(\n",
    "    file_pattern='guitar_dataset.tfrecord',\n",
    "    example_secs=4, #EXAMPLE_SECS PERAMETER?\n",
    "    sample_rate=16000 #SAMPLE_RATE PERAMETER?\n",
    ")\n",
    "\n",
    "#batches the dataset into sizes 1\n",
    "dataset = provider.get_dataset(batch_size=1)\n",
    "\n",
    "#grab first record\n",
    "example = next(iter(dataset))\n",
    "\n",
    "#example looks like this\n",
    "'''{\n",
    "  'audio': <Tensor shape=(1, 64000)>,  # 4s of audio at 16kHz\n",
    "  'f0_hz': <Tensor shape=(1, 1000)>,   # 250 Hz frame rate Ã— 4 seconds\n",
    "  'loudness_db': <Tensor shape=(1, 1000)>,\n",
    "  ...\n",
    "}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d55eeb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#with example, you can do this\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(example['f0_hz'][0].numpy(), label='Pitch (Hz)')\n",
    "plt.plot(example['loudness_db'][0].numpy(), label='Loudness (dB)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ea490",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Here's the actual model\n",
    "\n",
    "from ddsp.training.models import Autoencoder\n",
    "from ddsp.training.encoders import FcEncoder\n",
    "from ddsp.training.decoders import RnnFcDecoder\n",
    "\n",
    "model = Autoencoder(\n",
    "    encoder=FcEncoder(),       # [source](https://github.com/magenta/ddsp/blob/main/ddsp/training/encoders.py#L39)\n",
    "    decoder=RnnFcDecoder()     # [source](https://github.com/magenta/ddsp/blob/main/ddsp/training/decoders.py#L90)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36bdae2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Loss, comparing predicted spectrogram to actual spectrogram\n",
    "\n",
    "from ddsp.training.losses import MultiscaleSpectralLoss\n",
    "loss_fn = MultiscaleSpectralLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87f413",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Training loop\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(batch, training=True)\n",
    "        loss = loss_fn(batch['audio'], outputs['audio'])\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231022d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# train that bitch\n",
    "\n",
    "for step, batch in enumerate(dataset.take(1000)):\n",
    "    loss = train_step(batch)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss.numpy():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3effa3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Preprocess a new audio clip and then\n",
    "\n",
    "output = model(example, training=False)\n",
    "synth_audio = output['audio']\n",
    "\n",
    "\n",
    "\n",
    "import soundfile as sf\n",
    "sf.write(\"output.wav\", synth_audio.numpy()[0], 16000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toneTransfer",
   "language": "python",
   "name": "tonetransfer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
