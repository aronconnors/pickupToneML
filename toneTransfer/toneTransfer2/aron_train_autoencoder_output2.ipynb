{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4ea9f0e",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [6]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110f98d",
   "metadata": {
    "id": "SpXo6phTiOQM",
    "papermill": {
     "duration": 0.016523,
     "end_time": "2025-05-11T02:47:31.496679",
     "exception": false,
     "start_time": "2025-05-11T02:47:31.480156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train a DDSP Autoencoder on GPU\n",
    "\n",
    "This notebook demonstrates how to install the DDSP library and train it for synthesis based on your own data using our command-line scripts. If run inside of Colab, it will automatically use a free Google Cloud GPU.\n",
    "\n",
    "At the end, you'll have a custom-trained checkpoint that you can download to use with the [DDSP Timbre Transfer Colab](https://colab.research.google.com/github/magenta/ddsp/blob/main/ddsp/colab/demos/timbre_transfer.ipynb).\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/ddsp/additive_diagram/ddsp_autoencoder.png\" alt=\"DDSP Autoencoder figure\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea085e3",
   "metadata": {
    "id": "a4vmxpj1LC7m",
    "papermill": {
     "duration": 0.010675,
     "end_time": "2025-05-11T02:47:31.522035",
     "exception": false,
     "start_time": "2025-05-11T02:47:31.511360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Set your base directory\n",
    "* In drive, put all of the audio (.wav, .mp3) files with which you would like to train in a single folder.\n",
    " * Typically works well with 10-20 minutes of audio from a single monophonic source (also, one acoustic environment).\n",
    "* Use the file browser in the left panel to find a folder with your audio, right-click **\"Copy Path\", paste below**, and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba96b82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T02:47:31.548919Z",
     "iopub.status.busy": "2025-05-11T02:47:31.548340Z",
     "iopub.status.idle": "2025-05-11T02:47:31.567031Z",
     "shell.execute_reply": "2025-05-11T02:47:31.565779Z"
    },
    "id": "A0bK6P9DMBTb",
    "outputId": "e00c67c2-819b-447b-87c9-92a94f41c96d",
    "papermill": {
     "duration": 0.034837,
     "end_time": "2025-05-11T02:47:31.569720",
     "exception": false,
     "start_time": "2025-05-11T02:47:31.534883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive Folder Exists: monoGuitarDataset/singlecoil\n"
     ]
    }
   ],
   "source": [
    "#@markdown (ex. `/content/drive/My Drive/...`) Leave blank to skip loading from Drive.\n",
    "DRIVE_DIR = 'monoGuitarDataset/singlecoil' #@param {type: \"string\"}\n",
    "\n",
    "import os\n",
    "assert os.path.exists(DRIVE_DIR)\n",
    "print('Drive Folder Exists:', DRIVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19e60a",
   "metadata": {
    "id": "FELlizMtIxCH",
    "papermill": {
     "duration": 0.009665,
     "end_time": "2025-05-11T02:47:31.594387",
     "exception": false,
     "start_time": "2025-05-11T02:47:31.584722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Make directories to save model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66402b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T02:47:31.614037Z",
     "iopub.status.busy": "2025-05-11T02:47:31.613329Z",
     "iopub.status.idle": "2025-05-11T02:47:31.620669Z",
     "shell.execute_reply": "2025-05-11T02:47:31.619219Z"
    },
    "id": "Qd22WxEQI3FV",
    "papermill": {
     "duration": 0.018796,
     "end_time": "2025-05-11T02:47:31.622372",
     "exception": false,
     "start_time": "2025-05-11T02:47:31.603576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUDIO_DIR = 'data/audio'\n",
    "AUDIO_FILEPATTERN = AUDIO_DIR + '/*'\n",
    "#!mkdir -p $AUDIO_DIR\n",
    "\n",
    "SAVE_DIR = os.path.join(DRIVE_DIR, 'ddsp-solo-instrument')\n",
    "#!mkdir -p \"$SAVE_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89a3d1",
   "metadata": {
    "id": "fb4YD8woYD1H",
    "papermill": {
     "duration": 0.00928,
     "end_time": "2025-05-11T02:47:31.642188",
     "exception": false,
     "start_time": "2025-05-11T02:47:31.632908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2975fa5",
   "metadata": {
    "id": "uNhH7nEbX2db",
    "papermill": {
     "duration": 0.009593,
     "end_time": "2025-05-11T02:47:31.660810",
     "exception": false,
     "start_time": "2025-05-11T02:47:31.651217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Upload training audio\n",
    "\n",
    "Upload audio files to use for training your model. Uses `DRIVE_DIR` if connected to drive, otherwise prompts local upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc88fbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T02:47:31.682292Z",
     "iopub.status.busy": "2025-05-11T02:47:31.681625Z",
     "iopub.status.idle": "2025-05-11T02:47:31.688634Z",
     "shell.execute_reply": "2025-05-11T02:47:31.687232Z"
    },
    "id": "itVKEzF6m3rY",
    "outputId": "2111453e-8363-4657-8df6-b7129ea018b1",
    "papermill": {
     "duration": 0.020312,
     "end_time": "2025-05-11T02:47:31.690721",
     "exception": false,
     "start_time": "2025-05-11T02:47:31.670409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "#from ddsp.colab import colab_utils\n",
    "\n",
    "#mp3_files = glob.glob(os.path.join(DRIVE_DIR, '*.mp3'))\n",
    "#wav_files = glob.glob(os.path.join(DRIVE_DIR, '*.wav'))\n",
    "#audio_files = mp3_files + wav_files\n",
    "\n",
    "#for fname in audio_files:\n",
    "  #target_name = os.path.join(AUDIO_DIR, os.path.basename(fname).replace(' ', '_'))\n",
    "  #print('Copying {} to {}'.format(fname, target_name))\n",
    "  #!cp \"$fname\" $target_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19eedc",
   "metadata": {
    "id": "g_XVFoN2YOat",
    "papermill": {
     "duration": 0.009229,
     "end_time": "2025-05-11T02:47:31.711135",
     "exception": false,
     "start_time": "2025-05-11T02:47:31.701906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Preprocess raw audio into TFRecord dataset\n",
    "\n",
    "We need to do some preprocessing on the raw audio you uploaded to get it into the correct format for training. This involves turning the full audio into short (4-second) examples, inferring the fundamental frequency (or \"pitch\") with [CREPE](http://github.com/marl/crepe), and computing the loudness. These features will then be stored in a sharded [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) file for easier loading. Depending on the amount of input audio, this process usually takes a few minutes.\n",
    "\n",
    "* (Optional) Transfer dataset from drive. If you've already created a dataset, from a previous run, this cell will skip the dataset creation step and copy the dataset from `$DRIVE_DIR/data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "709c38a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-11T02:47:31.732118Z",
     "iopub.status.busy": "2025-05-11T02:47:31.731104Z",
     "iopub.status.idle": "2025-05-11T02:48:39.618103Z",
     "shell.execute_reply": "2025-05-11T02:48:39.616043Z"
    },
    "id": "MsnkAHyHVrCW",
    "outputId": "d008f97d-e512-4e81-89cf-74346cbd2b07",
    "papermill": {
     "duration": 67.901131,
     "end_time": "2025-05-11T02:48:39.621533",
     "exception": false,
     "start_time": "2025-05-11T02:47:31.720402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/erdos/csga/aconnors9/.conda/envs/guitar/lib/python3.8/site-packages/resampy/interpn.py:114: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 6103. The TBB threading layer is disabled.\n",
      "  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/erdos/csga/aconnors9/.conda/envs/guitar/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "#from ddsp.training.data_preparation import prepare_tfrecord\n",
    "\n",
    "TRAIN_TFRECORD = 'data/train.tfrecord'\n",
    "TRAIN_TFRECORD_FILEPATTERN = TRAIN_TFRECORD + '*'\n",
    "AUDIO_FILEPATTERN = 'data/audio/*'\n",
    "\n",
    "# Copy dataset from drive if dataset has already been created.\n",
    "#drive_data_dir = os.path.join(DRIVE_DIR, 'data/audio')\n",
    "#drive_dataset_files = glob.glob(drive_data_dir + '/*')\n",
    "\n",
    "#if DRIVE_DIR and len(drive_dataset_files) > 0:\n",
    "#  !cp \"$drive_data_dir\"/* data/\n",
    "\n",
    "#else:\n",
    "#  # Make a new dataset.\n",
    "#  if not glob.glob(AUDIO_FILEPATTERN):\n",
    "#    raise ValueError('No audio files found. Please use the previous cell to upload.')\n",
    "\n",
    "#audio_files = glob.glob(AUDIO_FILEPATTERN)\n",
    "#if not audio_files:\n",
    "#  raise ValueError('No audio files found in data/audio/. Please use the previous cell to upload.')\n",
    "\n",
    "import sys\n",
    "from absl import flags\n",
    "from ddsp.training.data_preparation import ddsp_prepare_tfrecord as prep_script\n",
    "\n",
    "#sys.argv = [\n",
    "#    'ddsp_prepare_tfrecord',\n",
    "#    '--input_audio_filepatterns=data/audio/*.wav',\n",
    "#    '--output_tfrecord_path=data/train.tfrecord',\n",
    "#    '--example_secs=1.5',\n",
    "#    '--sample_rate=16000',\n",
    "#    '--alsologtostderr'\n",
    "#]\n",
    "\n",
    "#app.run(prep_script.main)\n",
    "#flags.FLAGS(sys.argv)  # Required for some environments\n",
    "#flags.FLAGS.mark_as_parsed()\n",
    "#prep_script.main([])\n",
    "# Verify that TFRecord was created\n",
    "#tfrecords_created = glob.glob(TRAIN_TFRECORD_FILEPATTERN)\n",
    "#print(\"TFRecord files generated:\", tfrecords_created)\n",
    "#if not tfrecords_created:\n",
    "#    raise RuntimeError(\"TFRecord generation failed. No files created at data/train.tfrecord*.\")\n",
    "\n",
    "# Copy dataset to drive for safe-keeping\n",
    "#drive_tfrecord_dir = os.path.join(DRIVE_DIR, 'data')\n",
    "#os.makedirs(drive_tfrecord_dir, exist_ok=True)\n",
    "\n",
    "#print(f\"Saving TFRecords to: {drive_tfrecord_dir}\")\n",
    "import shutil\n",
    "\n",
    "#for tfrecord in tfrecords_created:\n",
    "#    shutil.copy(tfrecord, drive_tfrecord_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a1795",
   "metadata": {
    "id": "d4toX-D-AYZL",
    "papermill": {
     "duration": 0.037163,
     "end_time": "2025-05-11T02:48:39.721028",
     "exception": false,
     "start_time": "2025-05-11T02:48:39.683865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save dataset statistics for timbre transfer\n",
    "\n",
    "Quantile normalization helps match loudness of timbre transfer inputs to the\n",
    "loudness of the dataset, so let's calculate it here and save in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de595097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T02:48:39.795153Z",
     "iopub.status.busy": "2025-05-11T02:48:39.794130Z",
     "iopub.status.idle": "2025-05-11T02:48:39.814160Z",
     "shell.execute_reply": "2025-05-11T02:48:39.812625Z"
    },
    "papermill": {
     "duration": 0.065007,
     "end_time": "2025-05-11T02:48:39.817023",
     "exception": false,
     "start_time": "2025-05-11T02:48:39.752016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_parse(example):\n",
    "    try:\n",
    "        return True\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        return False\n",
    "\n",
    "def compute_dataset_statistics(data_provider, num_batches=100):\n",
    "    \"\"\"Compute mean/std stats from dataset, skipping corrupted records.\"\"\"\n",
    "    dataset = data_provider.get_dataset().shuffle(100)\n",
    "\n",
    "    loudness = []\n",
    "    f0_hz = []\n",
    "    f0_confidence = []\n",
    "\n",
    "    count = 0\n",
    "    for ex in dataset:\n",
    "        try:\n",
    "            loudness.append(ex['loudness_db'].numpy())\n",
    "            f0_hz.append(ex['f0_hz'].numpy())\n",
    "            f0_confidence.append(ex['f0_confidence'].numpy())\n",
    "            count += 1\n",
    "        except tf.errors.InvalidArgumentError as e:\n",
    "            print(\"Skipping example due to parse error:\", e)\n",
    "            continue\n",
    "\n",
    "        if count >= num_batches:\n",
    "            break\n",
    "\n",
    "    stats = {\n",
    "        'loudness_db': {\n",
    "            'mean': np.mean(np.concatenate(loudness)),\n",
    "            'std': np.std(np.concatenate(loudness)),\n",
    "        },\n",
    "        'f0_hz': {\n",
    "            'mean': np.mean(np.concatenate(f0_hz)),\n",
    "            'std': np.std(np.concatenate(f0_hz)),\n",
    "        },\n",
    "        'f0_confidence': {\n",
    "            'mean': np.mean(np.concatenate(f0_confidence)),\n",
    "            'std': np.std(np.concatenate(f0_confidence)),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a5f1a",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f729175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T02:48:39.890141Z",
     "iopub.status.busy": "2025-05-11T02:48:39.889568Z",
     "iopub.status.idle": "2025-05-11T02:48:58.709926Z",
     "shell.execute_reply": "2025-05-11T02:48:58.704103Z"
    },
    "id": "Bp_c8P0xApY6",
    "papermill": {
     "duration": 18.849246,
     "end_time": "2025-05-11T02:48:58.712724",
     "exception": true,
     "start_time": "2025-05-11T02:48:39.863478",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 22:48:40.060329: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-10 22:48:41.271419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30990 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2025-05-10 22:48:41.272288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30990 MB memory:  -> device: 1, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 22:48:41.746546: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2025-05-10 22:48:41.808068: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: loudness_db.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.808230: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: audio.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.808515: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_confidence.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.808579: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: audio.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.808791: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_hz.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.808897: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: loudness_db.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.809119: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: loudness_db.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.809161: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_confidence.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.809202: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: audio.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.809282: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_hz.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.809521: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_confidence.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.809636: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_hz.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.809715: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_hz.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.809820: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_confidence.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.809929: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: loudness_db.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.810126: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_hz.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.810166: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: audio.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.810202: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: loudness_db.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.810338: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: loudness_db.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.810407: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_hz.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.810564: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: loudness_db.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.810645: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_hz.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.810718: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: audio.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.810875: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_confidence.  Can't parse serialized Example.\n",
      "2025-05-10 22:48:41.810974: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at example_parsing_ops.cc:94 : Invalid argument: Key: f0_hz.  Can't parse serialized Example.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Key: loudness_db.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m PICKLE_FILE_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_statistics.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Compute stats\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_dataset_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_provider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Save to pickle file\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(PICKLE_FILE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mcompute_dataset_statistics\u001b[0;34m(data_provider, num_batches)\u001b[0m\n\u001b[1;32m     13\u001b[0m f0_confidence \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m         loudness\u001b[38;5;241m.\u001b[39mappend(ex[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloudness_db\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/.conda/envs/guitar/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:761\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    760\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/guitar/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 744\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/guitar/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py:2728\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2726\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   2727\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2728\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   2730\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/guitar/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6941\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m message \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6940\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 6941\u001b[0m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_status_to_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Key: loudness_db.  Can't parse serialized Example.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "import ddsp.training\n",
    "from ddsp.training.data import TFRecordProvider\n",
    "#from ddsp.training.data_preparation import compute_dataset_statistics\n",
    "#from ddsp.training.data_preparation.compute_statistics import compute_dataset_statistics\n",
    "#from ddsp.training.data_preparation.compute_statistics import compute_dataset_statistics\n",
    "#from ddsp.training.data_preparation import compute_dataset_statistics\n",
    "\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "data_provider = TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN)\n",
    "dataset = data_provider.get_dataset(shuffle=False)\n",
    "PICKLE_FILE_PATH = os.path.join(SAVE_DIR, 'dataset_statistics.pkl')\n",
    "\n",
    "# Compute stats\n",
    "stats = compute_dataset_statistics(data_provider)\n",
    "\n",
    "# Save to pickle file\n",
    "with open(PICKLE_FILE_PATH, 'wb') as f:\n",
    "    pickle.dump(stats, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a95ee5",
   "metadata": {
    "id": "nIsq0HrzbOF7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Let's load the dataset in the `ddsp` library and have a look at one of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce30bf7",
   "metadata": {
    "id": "dA-FOmRgYdpZ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ddsp.training\n",
    "from ddsp.training.data import TFRecordProvider\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "# Setup data provider\n",
    "print(TRAIN_TFRECORD_FILEPATTERN)\n",
    "data_provider = TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN)\n",
    "dataset = data_provider.get_dataset(shuffle=False)\n",
    "\n",
    "# Get one example\n",
    "try:\n",
    "    ex = next(iter(dataset))\n",
    "except StopIteration:\n",
    "    raise ValueError(\n",
    "        'TFRecord contains no examples. Please try re-running the pipeline with different audio file(s).')\n",
    "\n",
    "# === REPLACE colab_utils.specplot ===\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.specgram(ex['audio'].numpy(), Fs=16000)\n",
    "plt.title('Spectrogram')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# === REPLACE colab_utils.play ===\n",
    "# Save audio to file (so you can play it later on local machine)\n",
    "output_audio_path = os.path.join(SAVE_DIR, 'example_audio.wav')\n",
    "sf.write(output_audio_path, ex['audio'].numpy(), 16000)\n",
    "print(f\"âœ… Audio saved to {output_audio_path}\")\n",
    "\n",
    "# === Plot loudness, F0, confidence ===\n",
    "f, ax = plt.subplots(3, 1, figsize=(14, 4))\n",
    "x = np.linspace(0, 4.0, 1000)\n",
    "ax[0].set_ylabel('loudness_db')\n",
    "ax[0].plot(x, ex['loudness_db'])\n",
    "ax[1].set_ylabel('F0_Hz')\n",
    "ax[1].set_xlabel('seconds')\n",
    "ax[1].plot(x, ex['f0_hz'])\n",
    "ax[2].set_ylabel('F0_confidence')\n",
    "ax[2].set_xlabel('seconds')\n",
    "ax[2].plot(x, ex['f0_confidence'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dddfa8",
   "metadata": {
    "id": "9gvXBa7PbuyY",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Train Model\n",
    "\n",
    "We will now train a \"solo instrument\" model. This means the model is conditioned only on the fundamental frequency (f0) and loudness with no instrument ID or latent timbre feature. If you uploaded audio of multiple instruemnts, the neural network you train will attempt to model all timbres, but will likely associate certain timbres with different f0 and loudness conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e3712",
   "metadata": {
    "id": "YpwQkSIKjEMZ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "First, let's start up a [TensorBoard](https://www.tensorflow.org/tensorboard) to monitor our loss as training proceeds.\n",
    "\n",
    "Initially, TensorBoard will report `No dashboards are active for the current data set.`, but once training begins, the dashboards should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc3785",
   "metadata": {
    "id": "u2lx7yJneUXT",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "import tensorboard as tb\n",
    "tb.notebook.start('--logdir \"{}\"'.format(SAVE_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145fc57",
   "metadata": {
    "id": "fT-8Koyvj46w",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### We will now begin training.\n",
    "\n",
    "Note that we specify [gin configuration](https://github.com/google/gin-config) files for the both the model architecture ([solo_instrument.gin](TODO)) and the dataset ([tfrecord.gin](TODO)), which are both predefined in the library. You could also create your own. We then override some of the spefic params for `batch_size` (which is defined in in the model gin file) and the tfrecord path (which is defined in the dataset file).\n",
    "\n",
    "### Training Notes:\n",
    "* Models typically perform well when the loss drops to the range of ~4.5-5.0.\n",
    "* Depending on the dataset this can take anywhere from 5k-30k training steps usually.\n",
    "* The default is set to 30k, but you can stop training at any time, and for timbre transfer, it's best to stop before the loss drops too far below ~5.0 to avoid overfitting.\n",
    "* On the colab GPU, this can take from around 3-20 hours.\n",
    "* We **highly recommend** saving checkpoints directly to your drive account as colab will restart naturally after about 12 hours and you may lose all of your checkpoints.\n",
    "* By default, checkpoints will be saved every 300 steps with a maximum of 10 checkpoints (at ~60MB/checkpoint this is ~600MB). Feel free to adjust these numbers depending on the frequency of saves you would like and space on your drive.\n",
    "* If you're restarting a session and `DRIVE_DIR` points a directory that was previously used for training, training should resume at the last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb61739",
   "metadata": {
    "id": "poKO-mZEGYXZ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ddsp_run \\\n",
    "  --mode=train \\\n",
    "  --alsologtostderr \\\n",
    "  --save_dir=\"$SAVE_DIR\" \\\n",
    "  --gin_file=models/solo_instrument.gin \\\n",
    "  --gin_file=datasets/tfrecord.gin \\\n",
    "  --gin_param=\"TFRecordProvider.file_pattern='$TRAIN_TFRECORD_FILEPATTERN'\" \\\n",
    "  --gin_param=\"batch_size=16\" \\\n",
    "  --gin_param=\"train_util.train.num_steps=30000\" \\\n",
    "  --gin_param=\"train_util.train.steps_per_save=300\" \\\n",
    "  --gin_param=\"trainers.Trainer.checkpoints_to_keep=10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c89da0",
   "metadata": {
    "id": "V95qxVjFzWR6",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Resynthesis\n",
    "\n",
    "Check how well the model reconstructs the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4705db",
   "metadata": {
    "id": "OQ5PPDZVzgFR",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ddsp.training\n",
    "import gin\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "# Setup data provider\n",
    "data_provider = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN)\n",
    "dataset = data_provider.get_batch(batch_size=1, shuffle=False)\n",
    "\n",
    "# Get one batch\n",
    "try:\n",
    "    batch = next(iter(dataset))\n",
    "except StopIteration:\n",
    "    raise ValueError(\n",
    "        'TFRecord contains no examples. Please try re-running the pipeline with '\n",
    "        'different audio file(s).')\n",
    "\n",
    "# Parse the gin config\n",
    "gin_file = os.path.join(SAVE_DIR, 'operative_config-0.gin')\n",
    "gin.parse_config_file(gin_file)\n",
    "\n",
    "# Load model\n",
    "model = ddsp.training.models.Autoencoder()\n",
    "model.restore(SAVE_DIR)\n",
    "\n",
    "# Resynthesize audio\n",
    "outputs = model(batch, training=False)\n",
    "audio_gen = model.get_audio_from_outputs(outputs)\n",
    "audio = batch['audio']\n",
    "\n",
    "# === REPLACE specplot ===\n",
    "def plot_specgram(waveform, sample_rate=16000, title='Spectrogram'):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.specgram(waveform.numpy()[0], Fs=sample_rate)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.show()\n",
    "\n",
    "# === REPLACE play ===\n",
    "def save_audio(waveform, path, sample_rate=16000):\n",
    "    sf.write(path, waveform.numpy()[0], sample_rate)\n",
    "    print(f\"âœ… Saved audio to {path}\")\n",
    "\n",
    "# Plot and save original audio\n",
    "print('Original Audio')\n",
    "plot_specgram(audio)\n",
    "save_audio(audio, os.path.join(SAVE_DIR, 'original_audio.wav'))\n",
    "\n",
    "# Plot and save resynthesized audio\n",
    "print('Resynthesis')\n",
    "plot_specgram(audio_gen)\n",
    "save_audio(audio_gen, os.path.join(SAVE_DIR, 'resynthesis_audio.wav'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1bafd",
   "metadata": {
    "id": "ZXM2ynLQ2Wl3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Download Checkpoint\n",
    "\n",
    "Below you can download the final checkpoint. You are now ready to use it in the [DDSP Timbre Tranfer Colab](https://colab.research.google.com/github/magenta/ddsp/blob/main/ddsp/colab/demos/timbre_transfer.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709af3c9",
   "metadata": {
    "id": "2WDiCyXP0tNE",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Define filenames\n",
    "CHECKPOINT_ZIP = 'my_solo_instrument.zip'\n",
    "\n",
    "# Find latest checkpoint\n",
    "latest_checkpoint_path = tf.train.latest_checkpoint(SAVE_DIR)\n",
    "if latest_checkpoint_path is None:\n",
    "    raise ValueError(\"No checkpoint found in SAVE_DIR\")\n",
    "\n",
    "latest_checkpoint_fname = os.path.basename(latest_checkpoint_path)\n",
    "\n",
    "# Create the zip file\n",
    "zip_command = (\n",
    "    f'cd \"{SAVE_DIR}\" && zip {CHECKPOINT_ZIP} {latest_checkpoint_fname}* '\n",
    "    'operative_config-0.gin dataset_statistics.pkl'\n",
    ")\n",
    "subprocess.run(zip_command, shell=True, check=True)\n",
    "\n",
    "# Copy zip to current directory\n",
    "src = os.path.join(SAVE_DIR, CHECKPOINT_ZIP)\n",
    "dst = os.path.join('./', CHECKPOINT_ZIP)\n",
    "shutil.copy(src, dst)\n",
    "\n",
    "print(f\"âœ… Checkpoint ZIP saved as {dst}\")\n",
    "\n",
    "# Print instructions for manual download\n",
    "print(\"\\nðŸ“¦ To download the zip file to your local machine, run this from your laptop:\")\n",
    "print(f\"scp username@cluster:{os.getcwd()}/{CHECKPOINT_ZIP} .\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hMqWDc_m6rUC"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 91.669568,
   "end_time": "2025-05-11T02:49:00.587922",
   "environment_variables": {},
   "exception": true,
   "input_path": "aron_train_autoencoder2.ipynb",
   "output_path": "aron_train_autoencoder_output2.ipynb",
   "parameters": {},
   "start_time": "2025-05-11T02:47:28.918354",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}